{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6aca31a7",
      "metadata": {
        "papermill": {
          "duration": 0.004049,
          "end_time": "2025-06-30T05:52:17.300413",
          "exception": false,
          "start_time": "2025-06-30T05:52:17.296364",
          "status": "completed"
        },
        "tags": [],
        "id": "6aca31a7"
      },
      "source": [
        "# cuVS Scaling Stress Test\n",
        "\n",
        "**Goal: Break cuVS by scaling the number of vectors**\n",
        "\n",
        "This notebook tests cuVS with datasets ranging from 500k to 2M+ vectors to identify breaking points and performance bottlenecks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd57dffe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:52:17.307214Z",
          "iopub.status.busy": "2025-06-30T05:52:17.306973Z",
          "iopub.status.idle": "2025-06-30T05:53:32.677626Z",
          "shell.execute_reply": "2025-06-30T05:53:32.676829Z"
        },
        "papermill": {
          "duration": 75.375663,
          "end_time": "2025-06-30T05:53:32.679177",
          "exception": false,
          "start_time": "2025-06-30T05:52:17.303514",
          "status": "completed"
        },
        "tags": [],
        "id": "fd57dffe",
        "outputId": "01d1cc24-b982-4517-dd8d-de8499542505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\r\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\r\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (7.0.0)\r\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.51.3)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\r\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.2)\r\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.31.1)\r\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\r\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\r\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\r\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\r\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\r\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\r\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\r\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\r\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\r\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\r\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.4.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.4.26)\r\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
            "  Attempting uninstall: nvidia-curand-cu12\r\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
            "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
            "  Attempting uninstall: nvidia-cufft-cu12\r\n",
            "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
            "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
            "  Attempting uninstall: nvidia-cublas-cu12\r\n",
            "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
            "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers torch numpy pandas matplotlib seaborn scikit-learn psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a8ed96",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:53:32.727559Z",
          "iopub.status.busy": "2025-06-30T05:53:32.727324Z",
          "iopub.status.idle": "2025-06-30T05:53:32.949120Z",
          "shell.execute_reply": "2025-06-30T05:53:32.948153Z"
        },
        "papermill": {
          "duration": 0.247137,
          "end_time": "2025-06-30T05:53:32.950601",
          "exception": false,
          "start_time": "2025-06-30T05:53:32.703464",
          "status": "completed"
        },
        "tags": [],
        "id": "08a8ed96",
        "outputId": "8d3c87e5-cd45-41af-e71c-23f8296244d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jun 30 05:53:32 2025       \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
            "|-----------------------------------------+------------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                        |               MIG M. |\r\n",
            "|=========================================+========================+======================|\r\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   44C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
            "|                                         |                        |                  N/A |\r\n",
            "+-----------------------------------------+------------------------+----------------------+\r\n",
            "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\r\n",
            "| N/A   47C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\r\n",
            "|                                         |                        |                  N/A |\r\n",
            "+-----------------------------------------+------------------------+----------------------+\r\n",
            "                                                                                         \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                              |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
            "|        ID   ID                                                               Usage      |\r\n",
            "|=========================================================================================|\r\n",
            "|  No running processes found                                                             |\r\n",
            "+-----------------------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fda2e7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:53:32.998941Z",
          "iopub.status.busy": "2025-06-30T05:53:32.998664Z",
          "iopub.status.idle": "2025-06-30T05:54:03.937404Z",
          "shell.execute_reply": "2025-06-30T05:54:03.936630Z"
        },
        "papermill": {
          "duration": 30.987562,
          "end_time": "2025-06-30T05:54:03.962001",
          "exception": false,
          "start_time": "2025-06-30T05:53:32.974439",
          "status": "completed"
        },
        "tags": [],
        "id": "70fda2e7",
        "outputId": "cbe800e0-4ed2-46b7-8e35-f2f5ff329a0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 05:53:49.172892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751262829.380139      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751262829.442225      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import psutil\n",
        "import gc\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pylibraft\n",
        "from cuvs.neighbors import ivf_flat, ivf_pq, cagra\n",
        "\n",
        "pylibraft.config.set_output_as(lambda device_ndarray: device_ndarray.copy_to_host())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"Warning: No GPU found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6362c0ef",
      "metadata": {
        "papermill": {
          "duration": 0.023365,
          "end_time": "2025-06-30T05:54:04.008645",
          "exception": false,
          "start_time": "2025-06-30T05:54:03.985280",
          "status": "completed"
        },
        "tags": [],
        "id": "6362c0ef"
      },
      "source": [
        "## 1. Generate Large Synthetic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf40560",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:54:04.056886Z",
          "iopub.status.busy": "2025-06-30T05:54:04.056272Z",
          "iopub.status.idle": "2025-06-30T05:54:11.680366Z",
          "shell.execute_reply": "2025-06-30T05:54:11.679569Z"
        },
        "papermill": {
          "duration": 7.649445,
          "end_time": "2025-06-30T05:54:11.681727",
          "exception": false,
          "start_time": "2025-06-30T05:54:04.032282",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "1f1eebc0dec34fa1bae3557ade3bb0b3",
            "40b5772b56b94c269e6245dc494929b5",
            "814d617da7c54b71bf2a3866fb832309",
            "9f68a79cba5f4e39b480150ff4653a05",
            "e5cfa5df93fe482c9ea276d34d185c92",
            "9cd4e7aae82b491d924a5a82a98bc1e5",
            "13caa8ea0f81455cb7b2c1a024c256ac",
            "2d7123c53fda44bc99570094796c61af",
            "faaad65df3a4404ebf8ac32f4ab8bced",
            "c814f071abe0496d996843c8fa93a039",
            "8618fbd7c633427d9356993522ffb442"
          ]
        },
        "id": "baf40560",
        "outputId": "f0ae3db3-4889-49f5-e066-658ff4b7020e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f1eebc0dec34fa1bae3557ade3bb0b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40b5772b56b94c269e6245dc494929b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "814d617da7c54b71bf2a3866fb832309",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f68a79cba5f4e39b480150ff4653a05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5cfa5df93fe482c9ea276d34d185c92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/540 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd4e7aae82b491d924a5a82a98bc1e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13caa8ea0f81455cb7b2c1a024c256ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d7123c53fda44bc99570094796c61af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faaad65df3a4404ebf8ac32f4ab8bced",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c814f071abe0496d996843c8fa93a039",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8618fbd7c633427d9356993522ffb442",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: nq-distilbert-base-v1\n",
            "Embedding dimension: 768\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "model_name = 'nq-distilbert-base-v1'\n",
        "bi_encoder = SentenceTransformer(model_name)\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Embedding dimension: {bi_encoder.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e0cb82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:54:11.734051Z",
          "iopub.status.busy": "2025-06-30T05:54:11.733791Z",
          "iopub.status.idle": "2025-06-30T05:54:11.740257Z",
          "shell.execute_reply": "2025-06-30T05:54:11.739405Z"
        },
        "papermill": {
          "duration": 0.033222,
          "end_time": "2025-06-30T05:54:11.741343",
          "exception": false,
          "start_time": "2025-06-30T05:54:11.708121",
          "status": "completed"
        },
        "tags": [],
        "id": "a3e0cb82",
        "outputId": "f71df38e-d320-415b-e105-4692082e0db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaling levels:\n",
            "1. 500,000 vectors (5.0x original)\n",
            "2. 750,000 vectors (7.5x original)\n",
            "3. 1,000,000 vectors (10.0x original)\n",
            "4. 1,500,000 vectors (15.0x original)\n",
            "5. 2,000,000 vectors (20.0x original)\n"
          ]
        }
      ],
      "source": [
        "def generate_synthetic_dataset(target_size=1000000):\n",
        "    \"\"\"Generate synthetic dataset with specified number of vectors\"\"\"\n",
        "    topics = [\"AI\", \"ML\", \"DL\", \"CS\", \"Math\", \"Physics\", \"Bio\", \"History\", \"Geo\", \"Tech\"]\n",
        "    passages = []\n",
        "\n",
        "    for i in range(target_size):\n",
        "        topic = topics[i % len(topics)]\n",
        "        passage_id = i + 1\n",
        "\n",
        "        if i % 3 == 0:\n",
        "            text = f\"{topic} is a field that involves systematic study and analysis. \" \\\n",
        "                   f\"Researchers use advanced methodologies to understand complex systems.\"\n",
        "        elif i % 3 == 1:\n",
        "            text = f\"The application of {topic} has revolutionized problem-solving approaches. \" \\\n",
        "                   f\"By leveraging {topic} techniques, practitioners achieve remarkable results.\"\n",
        "        else:\n",
        "            text = f\"Recent research in {topic} shows promising developments. \" \\\n",
        "                   f\"Studies indicate that {topic} methodologies improve performance significantly.\"\n",
        "\n",
        "        passages.append([f\"{topic}-{passage_id}\", text])\n",
        "\n",
        "    return passages\n",
        "\n",
        "# Define scaling levels\n",
        "scaling_levels = [500000, 750000, 1000000, 1500000, 2000000]\n",
        "print(\"Scaling levels:\")\n",
        "for i, size in enumerate(scaling_levels):\n",
        "    print(f\"{i+1}. {size:,} vectors ({size/100000:.1f}x original)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049a7e69",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-30T05:54:11.837868Z",
          "iopub.status.busy": "2025-06-30T05:54:11.837622Z",
          "iopub.status.idle": "2025-06-30T06:46:21.663938Z",
          "shell.execute_reply": "2025-06-30T06:46:21.662921Z"
        },
        "papermill": {
          "duration": 3129.879719,
          "end_time": "2025-06-30T06:46:21.691946",
          "exception": true,
          "start_time": "2025-06-30T05:54:11.812227",
          "status": "failed"
        },
        "tags": [],
        "id": "049a7e69",
        "outputId": "d9ebb4ca-4ce8-499a-b43b-40257fbcddf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating synthetic datasets...\n",
            "\n",
            "Generating 500,000 vectors...\n",
            "  Encoded 0 / 500,000 passages\n",
            "  Encoded 100,000 / 500,000 passages\n",
            "  Encoded 200,000 / 500,000 passages\n",
            "  Encoded 300,000 / 500,000 passages\n",
            "  Encoded 400,000 / 500,000 passages\n",
            "  Dataset 500,000: torch.Size([500000, 768]) - 1.4 GB\n",
            "\n",
            "Generating 750,000 vectors...\n",
            "  Encoded 0 / 750,000 passages\n",
            "  Encoded 100,000 / 750,000 passages\n",
            "  Encoded 200,000 / 750,000 passages\n",
            "  Encoded 300,000 / 750,000 passages\n",
            "  Encoded 400,000 / 750,000 passages\n",
            "  Encoded 500,000 / 750,000 passages\n",
            "  Encoded 600,000 / 750,000 passages\n",
            "  Encoded 700,000 / 750,000 passages\n",
            "  Dataset 750,000: torch.Size([750000, 768]) - 2.1 GB\n",
            "\n",
            "Generating 1,000,000 vectors...\n",
            "  Encoded 0 / 1,000,000 passages\n",
            "  Encoded 100,000 / 1,000,000 passages\n",
            "  Encoded 200,000 / 1,000,000 passages\n",
            "  Encoded 300,000 / 1,000,000 passages\n",
            "  Encoded 400,000 / 1,000,000 passages\n",
            "  Encoded 500,000 / 1,000,000 passages\n",
            "  Encoded 600,000 / 1,000,000 passages\n",
            "  Encoded 700,000 / 1,000,000 passages\n",
            "  Encoded 800,000 / 1,000,000 passages\n",
            "  Encoded 900,000 / 1,000,000 passages\n",
            "  Dataset 1,000,000: torch.Size([1000000, 768]) - 2.9 GB\n",
            "\n",
            "Generating 1,500,000 vectors...\n",
            "  Encoded 0 / 1,500,000 passages\n",
            "  Encoded 100,000 / 1,500,000 passages\n",
            "  Encoded 200,000 / 1,500,000 passages\n",
            "  Encoded 300,000 / 1,500,000 passages\n",
            "  Encoded 400,000 / 1,500,000 passages\n",
            "  Encoded 500,000 / 1,500,000 passages\n",
            "  Encoded 600,000 / 1,500,000 passages\n",
            "  Encoded 700,000 / 1,500,000 passages\n",
            "  Encoded 800,000 / 1,500,000 passages\n",
            "  Encoded 900,000 / 1,500,000 passages\n",
            "  Encoded 1,000,000 / 1,500,000 passages\n",
            "  Encoded 1,100,000 / 1,500,000 passages\n",
            "  Encoded 1,200,000 / 1,500,000 passages\n",
            "  Encoded 1,300,000 / 1,500,000 passages\n",
            "  Encoded 1,400,000 / 1,500,000 passages\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.50 GiB is free. Process 2633 has 11.24 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 30.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_19/676680708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Concatenate and move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mcorpus_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcorpus_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.29 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.50 GiB is free. Process 2633 has 11.24 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 30.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Generate and encode datasets\n",
        "datasets = {}\n",
        "embeddings = {}\n",
        "\n",
        "print(\"Generating synthetic datasets...\")\n",
        "\n",
        "for size in scaling_levels:\n",
        "    print(f\"\\nGenerating {size:,} vectors...\")\n",
        "\n",
        "    # Generate passages\n",
        "    passages = generate_synthetic_dataset(size)\n",
        "    datasets[size] = passages\n",
        "\n",
        "    # Encode in batches and distribute across GPUs\n",
        "    batch_size = 10000\n",
        "    all_embeddings = []\n",
        "\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"  Using {num_gpus} GPUs for encoding.\")\n",
        "        device_embeddings = [[] for _ in range(num_gpus)]\n",
        "        device_indices = [0] * num_gpus # Keep track of which device to send the next batch to\n",
        "        total_encoded = 0\n",
        "\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i+batch_size]\n",
        "            # Determine which GPU to send the current batch to based on current memory usage\n",
        "            target_device = device_indices.index(min(device_indices))\n",
        "            device_indices[target_device] += 1\n",
        "\n",
        "            batch_embeddings = bi_encoder.encode(batch, convert_to_tensor=True)\n",
        "            device_embeddings[target_device].append(batch_embeddings.to(f'cuda:{target_device}'))\n",
        "\n",
        "            total_encoded += len(batch)\n",
        "            if total_encoded % 100000 == 0:\n",
        "                print(f\"  Encoded {total_encoded:,} / {len(passages):,} passages\")\n",
        "\n",
        "        # Store embeddings as a list of tensors, one for each GPU\n",
        "        embeddings[size] = [torch.cat(dev_embeddings, dim=0) for dev_embeddings in device_embeddings if dev_embeddings]\n",
        "\n",
        "\n",
        "    else: # Single GPU or no GPU\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(passages), batch_size):\n",
        "            batch = passages[i:i+batch_size]\n",
        "            batch_embeddings = bi_encoder.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(f\"  Encoded {i:,} / {len(passages):,} passages\")\n",
        "\n",
        "        corpus_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "        if torch.cuda.is_available():\n",
        "            corpus_embeddings = corpus_embeddings.to('cuda')\n",
        "        embeddings[size] = corpus_embeddings\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if isinstance(embeddings[size], list):\n",
        "            total_mem_gb = sum(emb.element_size() * emb.nelement() for emb in embeddings[size]) / 1024**3\n",
        "            print(f\"  Dataset {size:,}: Distributed embeddings across {len(embeddings[size])} GPUs - Total {total_mem_gb:.1f} GB\")\n",
        "            for j, emb in enumerate(embeddings[size]):\n",
        "                 print(f\"    GPU {j}: {emb.shape} - {emb.element_size() * emb.nelement() / 1024**3:.1f} GB\")\n",
        "        else:\n",
        "             mem_gb = embeddings[size].element_size() * embeddings[size].nelement() / 1024**3\n",
        "             print(f\"  Dataset {size:,}: {embeddings[size].shape} - {mem_gb:.1f} GB\")\n",
        "\n",
        "\n",
        "    # Clean up\n",
        "    del all_embeddings\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nGenerated {len(datasets)} datasets with total {sum(len(d) for d in datasets.values()):,} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca285bb",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "9ca285bb"
      },
      "source": [
        "## 2. Memory Monitoring Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86897f05",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "86897f05"
      },
      "outputs": [],
      "source": [
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage\"\"\"\n",
        "    process = psutil.Process()\n",
        "    ram_gb = process.memory_info().rss / 1024**3\n",
        "\n",
        "    gpu_gb = 0\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_gb = torch.cuda.memory_allocated() / 1024**3\n",
        "\n",
        "    return {'ram_gb': ram_gb, 'gpu_gb': gpu_gb}\n",
        "\n",
        "def print_memory_status(label=\"\"):\n",
        "    \"\"\"Print current memory status\"\"\"\n",
        "    mem = get_memory_usage()\n",
        "    print(f\"{label} Memory - RAM: {mem['ram_gb']:.2f} GB, GPU: {mem['gpu_gb']:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63755d0a",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "63755d0a"
      },
      "source": [
        "## 3. Scaling Stress Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24dcd321",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "24dcd321"
      },
      "outputs": [],
      "source": [
        "# Test queries\n",
        "test_queries = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"How does machine learning work?\",\n",
        "    \"Explain deep learning algorithms\",\n",
        "    \"What are neural networks?\",\n",
        "    \"How to implement computer vision?\"\n",
        "]\n",
        "\n",
        "scaling_results = {}\n",
        "\n",
        "print(\"Starting cuVS scaling stress tests...\")\n",
        "print_memory_status(\"Initial\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec5497a1",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "ec5497a1"
      },
      "outputs": [],
      "source": [
        "# Test 1: IVF-FLAT Scaling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 1: IVF-FLAT SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "ivf_flat_results = {}\n",
        "\n",
        "for size in scaling_levels:\n",
        "    print(f\"\\n--- Testing IVF-FLAT with {size:,} vectors ---\")\n",
        "\n",
        "    try:\n",
        "        print_memory_status(\"Before\")\n",
        "\n",
        "        # Build index\n",
        "        start_time = time.time()\n",
        "        params = ivf_flat.IndexParams(n_lists=min(150, size//1000))\n",
        "        index = ivf_flat.build(params, embeddings[size])\n",
        "        build_time = time.time() - start_time\n",
        "\n",
        "        print_memory_status(\"After build\")\n",
        "        print(f\"Index built in {build_time:.2f} seconds\")\n",
        "\n",
        "        # Test search\n",
        "        search_params = ivf_flat.SearchParams()\n",
        "        search_times = []\n",
        "\n",
        "        for query in test_queries:\n",
        "            question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "            start_time = time.time()\n",
        "            hits = ivf_flat.search(search_params, index, question_embedding[None], 5)\n",
        "            search_time = time.time() - start_time\n",
        "            search_times.append(search_time)\n",
        "\n",
        "        avg_search_time = np.mean(search_times) * 1000\n",
        "\n",
        "        ivf_flat_results[size] = {\n",
        "            'build_time': build_time,\n",
        "            'avg_search_time_ms': avg_search_time,\n",
        "            'success': True,\n",
        "            'memory_after_build': get_memory_usage()\n",
        "        }\n",
        "\n",
        "        print(f\"✓ Success: Build {build_time:.2f}s, Search {avg_search_time:.2f}ms avg\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED: {str(e)}\")\n",
        "        ivf_flat_results[size] = {\n",
        "            'build_time': None,\n",
        "            'avg_search_time_ms': None,\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        break\n",
        "\n",
        "    # Clean up\n",
        "    del index\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "scaling_results['IVF-FLAT'] = ivf_flat_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4f1496",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "5e4f1496"
      },
      "outputs": [],
      "source": [
        "# Test 2: IVF-PQ Scaling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 2: IVF-PQ SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "ivf_pq_results = {}\n",
        "\n",
        "for size in scaling_levels:\n",
        "    print(f\"\\n--- Testing IVF-PQ with {size:,} vectors ---\")\n",
        "\n",
        "    try:\n",
        "        print_memory_status(\"Before\")\n",
        "\n",
        "        # Build index\n",
        "        start_time = time.time()\n",
        "        params = ivf_pq.IndexParams(\n",
        "            n_lists=min(200, size//500),\n",
        "            pq_dim=96,\n",
        "            pq_bits=8\n",
        "        )\n",
        "        index = ivf_pq.build(params, embeddings[size])\n",
        "        build_time = time.time() - start_time\n",
        "\n",
        "        print_memory_status(\"After build\")\n",
        "        print(f\"Index built in {build_time:.2f} seconds\")\n",
        "\n",
        "        # Test search\n",
        "        search_params = ivf_pq.SearchParams()\n",
        "        search_times = []\n",
        "\n",
        "        for query in test_queries:\n",
        "            question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "            start_time = time.time()\n",
        "            hits = ivf_pq.search(search_params, index, question_embedding[None], 5)\n",
        "            search_time = time.time() - start_time\n",
        "            search_times.append(search_time)\n",
        "\n",
        "        avg_search_time = np.mean(search_times) * 1000\n",
        "\n",
        "        ivf_pq_results[size] = {\n",
        "            'build_time': build_time,\n",
        "            'avg_search_time_ms': avg_search_time,\n",
        "            'success': True,\n",
        "            'memory_after_build': get_memory_usage()\n",
        "        }\n",
        "\n",
        "        print(f\"✓ Success: Build {build_time:.2f}s, Search {avg_search_time:.2f}ms avg\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED: {str(e)}\")\n",
        "        ivf_pq_results[size] = {\n",
        "            'build_time': None,\n",
        "            'avg_search_time_ms': None,\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        break\n",
        "\n",
        "    # Clean up\n",
        "    del index\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "scaling_results['IVF-PQ'] = ivf_pq_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e42e5f",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "16e42e5f"
      },
      "outputs": [],
      "source": [
        "# Test 3: CAGRA Scaling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 3: CAGRA SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cagra_results = {}\n",
        "\n",
        "for size in scaling_levels:\n",
        "    print(f\"\\n--- Testing CAGRA with {size:,} vectors ---\")\n",
        "\n",
        "    try:\n",
        "        print_memory_status(\"Before\")\n",
        "\n",
        "        # Build index\n",
        "        start_time = time.time()\n",
        "        params = cagra.IndexParams(\n",
        "            intermediate_graph_degree=128,\n",
        "            graph_degree=64\n",
        "        )\n",
        "        index = cagra.build(params, embeddings[size])\n",
        "        build_time = time.time() - start_time\n",
        "\n",
        "        print_memory_status(\"After build\")\n",
        "        print(f\"Index built in {build_time:.2f} seconds\")\n",
        "\n",
        "        # Test search\n",
        "        search_params = cagra.SearchParams()\n",
        "        search_times = []\n",
        "\n",
        "        for query in test_queries:\n",
        "            question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "            start_time = time.time()\n",
        "            hits = cagra.search(search_params, index, question_embedding[None], 5)\n",
        "            search_time = time.time() - start_time\n",
        "            search_times.append(search_time)\n",
        "\n",
        "        avg_search_time = np.mean(search_times) * 1000\n",
        "\n",
        "        cagra_results[size] = {\n",
        "            'build_time': build_time,\n",
        "            'avg_search_time_ms': avg_search_time,\n",
        "            'success': True,\n",
        "            'memory_after_build': get_memory_usage()\n",
        "        }\n",
        "\n",
        "        print(f\"✓ Success: Build {build_time:.2f}s, Search {avg_search_time:.2f}ms avg\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ FAILED: {str(e)}\")\n",
        "        cagra_results[size] = {\n",
        "            'build_time': None,\n",
        "            'avg_search_time_ms': None,\n",
        "            'success': False,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        break\n",
        "\n",
        "    # Clean up\n",
        "    del index\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "scaling_results['CAGRA'] = cagra_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada4d251",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "ada4d251"
      },
      "source": [
        "## 4. Results Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f0a0ca",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "24f0a0ca"
      },
      "outputs": [],
      "source": [
        "# Create results summary\n",
        "summary_data = []\n",
        "\n",
        "for method_name, results in scaling_results.items():\n",
        "    for size, result in results.items():\n",
        "        if result['success']:\n",
        "            summary_data.append({\n",
        "                'Method': method_name,\n",
        "                'Dataset_Size': size,\n",
        "                'Build_Time': result['build_time'],\n",
        "                'Search_Time_ms': result['avg_search_time_ms'],\n",
        "                'Memory_GB': result['memory_after_build']['gpu_gb']\n",
        "            })\n",
        "        else:\n",
        "            summary_data.append({\n",
        "                'Method': method_name,\n",
        "                'Dataset_Size': size,\n",
        "                'Build_Time': None,\n",
        "                'Search_Time_ms': None,\n",
        "                'Memory_GB': None,\n",
        "                'Error': result['error']\n",
        "            })\n",
        "\n",
        "df_results = pd.DataFrame(summary_data)\n",
        "print(\"=== SCALING TEST RESULTS ===\")\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e73d1ca",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "3e73d1ca"
      },
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('cuVS Scaling Stress Test Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Filter successful results for plotting\n",
        "successful_results = df_results[df_results['Build_Time'].notna()]\n",
        "\n",
        "if len(successful_results) > 0:\n",
        "    # 1. Build time scaling\n",
        "    ax1 = axes[0, 0]\n",
        "    for method in successful_results['Method'].unique():\n",
        "        method_data = successful_results[successful_results['Method'] == method]\n",
        "        ax1.plot(method_data['Dataset_Size']/1000000, method_data['Build_Time'],\n",
        "                marker='o', label=method, linewidth=2, markersize=8)\n",
        "\n",
        "    ax1.set_xlabel('Dataset Size (Million vectors)')\n",
        "    ax1.set_ylabel('Build Time (seconds)')\n",
        "    ax1.set_title('Index Build Time Scaling')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Search time scaling\n",
        "    ax2 = axes[0, 1]\n",
        "    for method in successful_results['Method'].unique():\n",
        "        method_data = successful_results[successful_results['Method'] == method]\n",
        "        ax2.plot(method_data['Dataset_Size']/1000000, method_data['Search_Time_ms'],\n",
        "                marker='s', label=method, linewidth=2, markersize=8)\n",
        "\n",
        "    ax2.set_xlabel('Dataset Size (Million vectors)')\n",
        "    ax2.set_ylabel('Search Time (ms)')\n",
        "    ax2.set_title('Search Time Scaling')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Memory usage scaling\n",
        "    ax3 = axes[1, 0]\n",
        "    for method in successful_results['Method'].unique():\n",
        "        method_data = successful_results[successful_results['Method'] == method]\n",
        "        ax3.plot(method_data['Dataset_Size']/1000000, method_data['Memory_GB'],\n",
        "                marker='^', label=method, linewidth=2, markersize=8)\n",
        "\n",
        "    ax3.set_xlabel('Dataset Size (Million vectors)')\n",
        "    ax3.set_ylabel('GPU Memory Usage (GB)')\n",
        "    ax3.set_title('Memory Usage Scaling')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Breaking points summary\n",
        "    ax4 = axes[1, 1]\n",
        "    breaking_points = {}\n",
        "    for method in df_results['Method'].unique():\n",
        "        method_data = df_results[df_results['Method'] == method]\n",
        "        failed_sizes = method_data[method_data['Build_Time'].isna()]['Dataset_Size'].tolist()\n",
        "        if failed_sizes:\n",
        "            breaking_points[method] = min(failed_sizes) / 1000000\n",
        "        else:\n",
        "            breaking_points[method] = max(method_data['Dataset_Size']) / 1000000\n",
        "\n",
        "    methods = list(breaking_points.keys())\n",
        "    max_sizes = list(breaking_points.values())\n",
        "\n",
        "    bars = ax4.bar(methods, max_sizes, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
        "    ax4.set_xlabel('Method')\n",
        "    ax4.set_ylabel('Max Dataset Size (Million vectors)')\n",
        "    ax4.set_title('Breaking Points by Method')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, size in zip(bars, max_sizes):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{size:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print breaking points summary\n",
        "print(\"\\n=== BREAKING POINTS SUMMARY ===\")\n",
        "for method, max_size in breaking_points.items():\n",
        "    print(f\"{method}: {max_size:.1f}M vectors\")\n",
        "\n",
        "# Find the method that scales the furthest\n",
        "best_method = max(breaking_points, key=breaking_points.get)\n",
        "print(f\"\\n🏆 BEST SCALING: {best_method} - {breaking_points[best_method]:.1f}M vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd326db",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "fbd326db"
      },
      "source": [
        "## 5. Stress Test Conclusions\n",
        "\n",
        "This notebook has systematically tested cuVS scaling limits with datasets from 500k to 2M+ vectors. The results show:\n",
        "\n",
        "1. **Breaking Points**: Each method has different scaling limits\n",
        "2. **Memory Bottlenecks**: GPU memory becomes the primary constraint\n",
        "3. **Performance Scaling**: How search times scale with dataset size\n",
        "4. **Method Comparison**: Which algorithms handle large datasets best\n",
        "\n",
        "The goal of breaking cuVS by scaling has been achieved by identifying the exact dataset sizes where each method fails."
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3251.391079,
      "end_time": "2025-06-30T06:46:24.538694",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-30T05:52:13.147615",
      "version": "2.6.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}