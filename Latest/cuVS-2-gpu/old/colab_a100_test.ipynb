{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Multi-GPU RAG Testing on Google Colab A100\n",
    "\n",
    "This notebook tests the improved parallel GPU implementation with:\n",
    "- Parallel GPU execution\n",
    "- Top-2K retrieval for better recall\n",
    "- Enhanced error handling\n",
    "- FAISS comparison benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and GPU Check\n",
    "\n",
    "First, ensure you have selected A100 GPU in Runtime > Change runtime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and type\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cuVS and dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install pylibraft-cu12 cuvs-cu12\n",
    "!pip install sentence-transformers torch numpy pandas matplotlib seaborn psutil\n",
    "!pip install faiss-gpu  # For comparison benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Improved Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the improved implementation\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from enum import Enum\n",
    "import gc\n",
    "import psutil\n",
    "from contextlib import contextmanager\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the improved implementation classes here\n",
    "# (Include all classes from improved_multi_gpu_rag.py)\n",
    "\n",
    "class IndexType(Enum):\n",
    "    \"\"\"Supported index types\"\"\"\n",
    "    IVF_FLAT = \"ivf_flat\"\n",
    "    IVF_PQ = \"ivf_pq\"\n",
    "    CAGRA = \"cagra\"\n",
    "    FAISS_FLAT = \"faiss_flat\"\n",
    "    FAISS_IVF = \"faiss_ivf\"\n",
    "\n",
    "@dataclass\n",
    "class SearchConfig:\n",
    "    \"\"\"Configuration for search operations\"\"\"\n",
    "    top_k: int = 2000  # Changed from 5 to 2000 for better recall\n",
    "    search_batch_size: int = 100\n",
    "    num_queries: int = 100\n",
    "    enable_recall_eval: bool = True\n",
    "    recall_k_values: List[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.recall_k_values is None:\n",
    "            self.recall_k_values = [1, 5, 10, 50, 100, 500, 1000, 2000]\n",
    "\n",
    "# Include rest of the classes from improved_multi_gpu_rag.py\n",
    "# ... (GPUConfig, CUDAMemoryManager, ParallelIndexBuilder, ParallelSearchEngine, RecallEvaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Start with smaller model for testing\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Generate synthetic dataset\n",
    "def generate_synthetic_texts(num_texts: int) -> List[str]:\n",
    "    \"\"\"Generate synthetic texts for testing\"\"\"\n",
    "    topics = [\"AI\", \"ML\", \"DL\", \"NLP\", \"CV\", \"RL\", \"Data\", \"Cloud\", \"IoT\", \"Blockchain\"]\n",
    "    templates = [\n",
    "        \"Research in {topic} shows {finding} with {application}\",\n",
    "        \"Advanced {topic} techniques enable {capability} for {domain}\",\n",
    "        \"The future of {topic} involves {trend} and {innovation}\",\n",
    "        \"Understanding {topic} requires {skill} and {knowledge}\",\n",
    "        \"Applications of {topic} include {use1} and {use2}\"\n",
    "    ]\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(num_texts):\n",
    "        topic = topics[i % len(topics)]\n",
    "        template = templates[i % len(templates)]\n",
    "        text = template.format(\n",
    "            topic=topic,\n",
    "            finding=f\"finding_{i}\",\n",
    "            application=f\"app_{i}\",\n",
    "            capability=f\"cap_{i}\",\n",
    "            domain=f\"domain_{i}\",\n",
    "            trend=f\"trend_{i}\",\n",
    "            innovation=f\"innovation_{i}\",\n",
    "            skill=f\"skill_{i}\",\n",
    "            knowledge=f\"knowledge_{i}\",\n",
    "            use1=f\"use1_{i}\",\n",
    "            use2=f\"use2_{i}\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# Generate embeddings\n",
    "num_vectors = 500000  # Start with 500K for testing\n",
    "batch_size = 1000\n",
    "\n",
    "print(f\"Generating {num_vectors:,} synthetic texts...\")\n",
    "texts = generate_synthetic_texts(num_vectors)\n",
    "\n",
    "print(\"Encoding texts to embeddings...\")\n",
    "embeddings = model.encode(\n",
    "    texts, \n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Memory used: {embeddings.element_size() * embeddings.nelement() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Parallel cuVS Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylibraft\n",
    "from cuvs.neighbors import ivf_flat, ivf_pq, cagra\n",
    "\n",
    "pylibraft.config.set_output_as(lambda device_ndarray: device_ndarray.copy_to_host())\n",
    "\n",
    "# For single GPU (Colab usually has 1 GPU), we'll simulate multi-GPU by splitting data\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "# Split embeddings for simulated multi-GPU processing\n",
    "if num_gpus == 1:\n",
    "    # Simulate 2 GPUs by splitting data\n",
    "    print(\"Single GPU detected - simulating 2-GPU setup by splitting data\")\n",
    "    split_size = len(embeddings) // 2\n",
    "    embedding_parts = [\n",
    "        embeddings[:split_size].cuda(),\n",
    "        embeddings[split_size:].cuda()\n",
    "    ]\n",
    "    simulated_gpus = 2\n",
    "else:\n",
    "    # True multi-GPU setup\n",
    "    chunks = torch.chunk(embeddings, num_gpus, dim=0)\n",
    "    embedding_parts = [chunk.to(f'cuda:{i}') for i, chunk in enumerate(chunks)]\n",
    "    simulated_gpus = num_gpus\n",
    "\n",
    "print(f\"Data split into {len(embedding_parts)} parts\")\n",
    "for i, part in enumerate(embedding_parts):\n",
    "    print(f\"  Part {i}: {part.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different index types with new parallel implementation\n",
    "results_comparison = {}\n",
    "\n",
    "# Configuration\n",
    "search_config = SearchConfig(\n",
    "    top_k=2000,  # Using 2K for better recall\n",
    "    search_batch_size=10,\n",
    "    num_queries=100,\n",
    "    enable_recall_eval=True\n",
    ")\n",
    "\n",
    "index_types = [IndexType.IVF_FLAT, IndexType.IVF_PQ, IndexType.CAGRA]\n",
    "\n",
    "for index_type in index_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {index_type.value.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        # Build indices (simulated parallel for single GPU)\n",
    "        print(f\"Building {index_type.value} indices...\")\n",
    "        \n",
    "        indices = []\n",
    "        build_times = []\n",
    "        \n",
    "        for i, part in enumerate(embedding_parts):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if index_type == IndexType.IVF_FLAT:\n",
    "                params = ivf_flat.IndexParams(\n",
    "                    n_lists=min(256, part.shape[0] // 1000 + 1)\n",
    "                )\n",
    "                index = ivf_flat.build(params, part)\n",
    "                \n",
    "            elif index_type == IndexType.IVF_PQ:\n",
    "                params = ivf_pq.IndexParams(\n",
    "                    n_lists=min(512, part.shape[0] // 500 + 1),\n",
    "                    pq_dim=min(96, part.shape[1] // 2),\n",
    "                    pq_bits=8\n",
    "                )\n",
    "                index = ivf_pq.build(params, part)\n",
    "                \n",
    "            elif index_type == IndexType.CAGRA:\n",
    "                params = cagra.IndexParams(\n",
    "                    intermediate_graph_degree=64,  # Reduced for memory\n",
    "                    graph_degree=32\n",
    "                )\n",
    "                index = cagra.build(params, part)\n",
    "            \n",
    "            build_time = time.time() - start_time\n",
    "            indices.append(index)\n",
    "            build_times.append(build_time)\n",
    "            print(f\"  Part {i} index built in {build_time:.2f}s\")\n",
    "        \n",
    "        total_build_time = sum(build_times)\n",
    "        print(f\"Total build time: {total_build_time:.2f}s\")\n",
    "        \n",
    "        # Test search with top-2K retrieval\n",
    "        print(f\"\\nTesting search with top-{search_config.top_k} retrieval...\")\n",
    "        \n",
    "        # Generate test queries\n",
    "        test_query_texts = [\n",
    "            \"Advanced machine learning techniques\",\n",
    "            \"Deep learning applications in healthcare\",\n",
    "            \"Natural language processing research\",\n",
    "            \"Computer vision for autonomous vehicles\",\n",
    "            \"Reinforcement learning in robotics\"\n",
    "        ]\n",
    "        \n",
    "        query_embeddings = model.encode(\n",
    "            test_query_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device='cuda'\n",
    "        )\n",
    "        \n",
    "        search_times = []\n",
    "        all_results = []\n",
    "        \n",
    "        for q_idx, query in enumerate(query_embeddings):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Search all indices\n",
    "            all_distances = []\n",
    "            all_indices_results = []\n",
    "            \n",
    "            for idx, index in enumerate(indices):\n",
    "                if index_type == IndexType.IVF_FLAT:\n",
    "                    search_params = ivf_flat.SearchParams()\n",
    "                    distances, indices_res = ivf_flat.search(\n",
    "                        search_params, index, query.unsqueeze(0), search_config.top_k\n",
    "                    )\n",
    "                elif index_type == IndexType.IVF_PQ:\n",
    "                    search_params = ivf_pq.SearchParams()\n",
    "                    distances, indices_res = ivf_pq.search(\n",
    "                        search_params, index, query.unsqueeze(0), search_config.top_k\n",
    "                    )\n",
    "                elif index_type == IndexType.CAGRA:\n",
    "                    search_params = cagra.SearchParams()\n",
    "                    distances, indices_res = cagra.search(\n",
    "                        search_params, index, query.unsqueeze(0), search_config.top_k\n",
    "                    )\n",
    "                \n",
    "                # Adjust indices for global indexing\n",
    "                offset = idx * (len(embeddings) // len(embedding_parts))\n",
    "                indices_res = indices_res + offset\n",
    "                \n",
    "                all_distances.extend(distances.flatten())\n",
    "                all_indices_results.extend(indices_res.flatten())\n",
    "            \n",
    "            # Merge and get top-2K\n",
    "            all_distances = np.array(all_distances)\n",
    "            all_indices_results = np.array(all_indices_results)\n",
    "            \n",
    "            sorted_idx = np.argsort(all_distances)[:search_config.top_k]\n",
    "            final_distances = all_distances[sorted_idx]\n",
    "            final_indices = all_indices_results[sorted_idx]\n",
    "            \n",
    "            search_time = time.time() - start_time\n",
    "            search_times.append(search_time)\n",
    "            all_results.append((final_distances, final_indices))\n",
    "            \n",
    "            print(f\"  Query {q_idx+1}: {search_time*1000:.2f}ms, retrieved {len(final_indices)} results\")\n",
    "        \n",
    "        avg_search_time = np.mean(search_times) * 1000\n",
    "        print(f\"\\nAverage search time: {avg_search_time:.2f}ms\")\n",
    "        \n",
    "        results_comparison[index_type.value] = {\n",
    "            'build_time': total_build_time,\n",
    "            'avg_search_time_ms': avg_search_time,\n",
    "            'num_results': len(final_indices),\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Cleanup\n",
    "        del indices\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {index_type.value}: {e}\")\n",
    "        results_comparison[index_type.value] = {\n",
    "            'build_time': None,\n",
    "            'avg_search_time_ms': None,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FAISS Comparison Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAISS COMPARISON BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert embeddings to numpy for FAISS\n",
    "embeddings_np = embeddings.cpu().numpy().astype('float32')\n",
    "\n",
    "# Test FAISS Flat (exact search)\n",
    "print(\"\\nTesting FAISS Flat Index...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Build FAISS index\n",
    "index_flat = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index_flat_gpu = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "    index_flat_gpu.add(embeddings_np)\n",
    "    faiss_index = index_flat_gpu\n",
    "else:\n",
    "    index_flat.add(embeddings_np)\n",
    "    faiss_index = index_flat\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "print(f\"FAISS Flat build time: {build_time:.2f}s\")\n",
    "\n",
    "# Test search\n",
    "query_embeddings_np = query_embeddings.cpu().numpy().astype('float32')\n",
    "search_times = []\n",
    "\n",
    "for query in query_embeddings_np:\n",
    "    start_time = time.time()\n",
    "    distances, indices = faiss_index.search(query.reshape(1, -1), search_config.top_k)\n",
    "    search_time = time.time() - start_time\n",
    "    search_times.append(search_time)\n",
    "\n",
    "avg_search_time = np.mean(search_times) * 1000\n",
    "print(f\"FAISS Flat average search time: {avg_search_time:.2f}ms\")\n",
    "\n",
    "results_comparison['faiss_flat'] = {\n",
    "    'build_time': build_time,\n",
    "    'avg_search_time_ms': avg_search_time,\n",
    "    'num_results': search_config.top_k,\n",
    "    'success': True\n",
    "}\n",
    "\n",
    "# Test FAISS IVF\n",
    "print(\"\\nTesting FAISS IVF Index...\")\n",
    "start_time = time.time()\n",
    "\n",
    "nlist = 256\n",
    "quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    index_ivf_gpu = faiss.index_cpu_to_gpu(res, 0, index_ivf)\n",
    "    index_ivf_gpu.train(embeddings_np)\n",
    "    index_ivf_gpu.add(embeddings_np)\n",
    "    faiss_ivf_index = index_ivf_gpu\n",
    "else:\n",
    "    index_ivf.train(embeddings_np)\n",
    "    index_ivf.add(embeddings_np)\n",
    "    faiss_ivf_index = index_ivf\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "print(f\"FAISS IVF build time: {build_time:.2f}s\")\n",
    "\n",
    "# Set search parameters\n",
    "faiss_ivf_index.nprobe = 16\n",
    "\n",
    "# Test search\n",
    "search_times = []\n",
    "for query in query_embeddings_np:\n",
    "    start_time = time.time()\n",
    "    distances, indices = faiss_ivf_index.search(query.reshape(1, -1), search_config.top_k)\n",
    "    search_time = time.time() - start_time\n",
    "    search_times.append(search_time)\n",
    "\n",
    "avg_search_time = np.mean(search_times) * 1000\n",
    "print(f\"FAISS IVF average search time: {avg_search_time:.2f}ms\")\n",
    "\n",
    "results_comparison['faiss_ivf'] = {\n",
    "    'build_time': build_time,\n",
    "    'avg_search_time_ms': avg_search_time,\n",
    "    'num_results': search_config.top_k,\n",
    "    'success': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results_comparison, orient='index')\n",
    "results_df.index.name = 'Index Type'\n",
    "results_df = results_df.reset_index()\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Filter successful results for plotting\n",
    "successful_results = results_df[results_df['success'] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if len(successful_results) > 0:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Build time comparison\n",
    "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "    bars1 = ax1.bar(successful_results['Index Type'], successful_results['build_time'], color=colors[:len(successful_results)])\n",
    "    ax1.set_xlabel('Index Type')\n",
    "    ax1.set_ylabel('Build Time (seconds)')\n",
    "    ax1.set_title('Index Build Time Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # Search time comparison\n",
    "    bars2 = ax2.bar(successful_results['Index Type'], successful_results['avg_search_time_ms'], color=colors[:len(successful_results)])\n",
    "    ax2.set_xlabel('Index Type')\n",
    "    ax2.set_ylabel('Average Search Time (ms)')\n",
    "    ax2.set_title(f'Search Time Comparison (top-{search_config.top_k})')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}ms', ha='center', va='bottom')\n",
    "    \n",
    "    # Speed vs Accuracy trade-off (using build time as proxy for accuracy)\n",
    "    ax3.scatter(successful_results['build_time'], successful_results['avg_search_time_ms'],\n",
    "               s=200, c=colors[:len(successful_results)], alpha=0.6)\n",
    "    \n",
    "    for i, row in successful_results.iterrows():\n",
    "        ax3.annotate(row['Index Type'],\n",
    "                    (row['build_time'], row['avg_search_time_ms']),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax3.set_xlabel('Build Time (seconds)')\n",
    "    ax3.set_ylabel('Search Time (ms)')\n",
    "    ax3.set_title('Build Time vs Search Time Trade-off')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find best performers\n",
    "    fastest_build = successful_results.loc[successful_results['build_time'].idxmin()]\n",
    "    fastest_search = successful_results.loc[successful_results['avg_search_time_ms'].idxmin()]\n",
    "    \n",
    "    print(f\"Fastest Build: {fastest_build['Index Type']} ({fastest_build['build_time']:.2f}s)\")\n",
    "    print(f\"Fastest Search: {fastest_search['Index Type']} ({fastest_search['avg_search_time_ms']:.2f}ms)\")\n",
    "    \n",
    "    # Compare cuVS vs FAISS\n",
    "    cuvs_indices = ['ivf_flat', 'ivf_pq', 'cagra']\n",
    "    faiss_indices = ['faiss_flat', 'faiss_ivf']\n",
    "    \n",
    "    cuvs_results = successful_results[successful_results['Index Type'].isin(cuvs_indices)]\n",
    "    faiss_results = successful_results[successful_results['Index Type'].isin(faiss_indices)]\n",
    "    \n",
    "    if len(cuvs_results) > 0 and len(faiss_results) > 0:\n",
    "        print(\"\\ncuVS vs FAISS:\")\n",
    "        print(f\"  cuVS avg build time: {cuvs_results['build_time'].mean():.2f}s\")\n",
    "        print(f\"  FAISS avg build time: {faiss_results['build_time'].mean():.2f}s\")\n",
    "        print(f\"  cuVS avg search time: {cuvs_results['avg_search_time_ms'].mean():.2f}ms\")\n",
    "        print(f\"  FAISS avg search time: {faiss_results['avg_search_time_ms'].mean():.2f}ms\")\n",
    "        \n",
    "        # Calculate speedup\n",
    "        faiss_avg_search = faiss_results['avg_search_time_ms'].mean()\n",
    "        cuvs_avg_search = cuvs_results['avg_search_time_ms'].mean()\n",
    "        \n",
    "        if cuvs_avg_search < faiss_avg_search:\n",
    "            speedup = faiss_avg_search / cuvs_avg_search\n",
    "            print(f\"\\ncuVS is {speedup:.2f}x faster than FAISS in search\")\n",
    "        else:\n",
    "            speedup = cuvs_avg_search / faiss_avg_search\n",
    "            print(f\"\\nFAISS is {speedup:.2f}x faster than cuVS in search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_info():\n",
    "    \"\"\"Get detailed GPU memory information\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Total memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "            print(f\"  Free: {(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"Current GPU Memory Status:\")\n",
    "get_gpu_memory_info()\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nAfter cleanup:\")\n",
    "get_gpu_memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Scaling Test with Different Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scaling with different dataset sizes\n",
    "scaling_sizes = [100000, 250000, 500000, 750000, 1000000]\n",
    "scaling_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCALING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for size in scaling_sizes:\n",
    "    print(f\"\\nTesting with {size:,} vectors...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate embeddings\n",
    "        test_texts = generate_synthetic_texts(size)\n",
    "        test_embeddings = model.encode(\n",
    "            test_texts[:size],\n",
    "            batch_size=1000,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=False,\n",
    "            device='cuda'\n",
    "        )\n",
    "        \n",
    "        # Build IVF-Flat index (fastest)\n",
    "        start_time = time.time()\n",
    "        params = ivf_flat.IndexParams(n_lists=min(256, test_embeddings.shape[0] // 1000 + 1))\n",
    "        index = ivf_flat.build(params, test_embeddings)\n",
    "        build_time = time.time() - start_time\n",
    "        \n",
    "        # Test search\n",
    "        query = test_embeddings[0].unsqueeze(0)\n",
    "        start_time = time.time()\n",
    "        search_params = ivf_flat.SearchParams()\n",
    "        distances, indices = ivf_flat.search(search_params, index, query, min(2000, size // 10))\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        scaling_results.append({\n",
    "            'size': size,\n",
    "            'build_time': build_time,\n",
    "            'search_time_ms': search_time * 1000,\n",
    "            'success': True\n",
    "        })\n",
    "        \n",
    "        print(f\"  Build: {build_time:.2f}s, Search: {search_time*1000:.2f}ms\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del test_embeddings, index\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed at {size:,} vectors: {e}\")\n",
    "        scaling_results.append({\n",
    "            'size': size,\n",
    "            'build_time': None,\n",
    "            'search_time_ms': None,\n",
    "            'success': False\n",
    "        })\n",
    "        break\n",
    "\n",
    "# Plot scaling results\n",
    "scaling_df = pd.DataFrame(scaling_results)\n",
    "successful_scaling = scaling_df[scaling_df['success'] == True]\n",
    "\n",
    "if len(successful_scaling) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Build time scaling\n",
    "    ax1.plot(successful_scaling['size']/1000, successful_scaling['build_time'],\n",
    "            'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "    ax1.set_xlabel('Dataset Size (thousands)')\n",
    "    ax1.set_ylabel('Build Time (seconds)')\n",
    "    ax1.set_title('Build Time Scaling')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Search time scaling\n",
    "    ax2.plot(successful_scaling['size']/1000, successful_scaling['search_time_ms'],\n",
    "            's-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "    ax2.set_xlabel('Dataset Size (thousands)')\n",
    "    ax2.set_ylabel('Search Time (ms)')\n",
    "    ax2.set_title('Search Time Scaling (top-2K)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMaximum successful size: {successful_scaling['size'].max():,} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results for SLURM Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for analysis\n",
    "results_df.to_csv('colab_a100_results.csv', index=False)\n",
    "scaling_df.to_csv('colab_a100_scaling_results.csv', index=False)\n",
    "\n",
    "print(\"Results saved to:\")\n",
    "print(\"  - colab_a100_results.csv\")\n",
    "print(\"  - colab_a100_scaling_results.csv\")\n",
    "\n",
    "# Generate SLURM job script\n",
    "slurm_script = '''#!/bin/bash\n",
    "#SBATCH --job-name=rag_multi_gpu_test\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:a100:2\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --output=rag_test_%j.out\n",
    "#SBATCH --error=rag_test_%j.err\n",
    "\n",
    "# Load modules\n",
    "module --force purge\n",
    "module load StdEnv/2023 gcc/12.3 cuda/12.2 python/3.11\n",
    "\n",
    "# Activate virtual environment\n",
    "source ~/rag_env/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install pylibraft-cu12 cuvs-cu12\n",
    "pip install sentence-transformers torch numpy pandas matplotlib\n",
    "\n",
    "# Run the improved implementation\n",
    "python improved_multi_gpu_rag.py\n",
    "\n",
    "echo \"Job completed at $(date)\"\n",
    "'''\n",
    "\n",
    "with open('submit_rag_test.sh', 'w') as f:\n",
    "    f.write(slurm_script)\n",
    "\n",
    "print(\"\\nSLURM job script saved to: submit_rag_test.sh\")\n",
    "print(\"To submit on Narval: sbatch submit_rag_test.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}