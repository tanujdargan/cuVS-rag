{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8969b0c-b2bc-47d0-bb24-20b06f0e63c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:16:50.726609Z",
     "iopub.status.busy": "2025-08-21T18:16:50.726378Z",
     "iopub.status.idle": "2025-08-21T18:18:04.443695Z",
     "shell.execute_reply": "2025-08-21T18:18:04.442853Z",
     "shell.execute_reply.started": "2025-08-21T18:16:50.726583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fbd6311a590>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-gpu/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fbd6311a890>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/faiss-gpu/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d670e9f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-21T18:18:04.445818Z",
     "iopub.status.busy": "2025-08-21T18:18:04.445559Z",
     "iopub.status.idle": "2025-08-21T18:18:13.565493Z",
     "shell.execute_reply": "2025-08-21T18:18:13.564636Z",
     "shell.execute_reply.started": "2025-08-21T18:18:04.445790Z"
    },
    "id": "d670e9f8",
    "outputId": "6a705709-f33c-4c81-fc60-a418766faa2e",
    "trusted": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (4.46.1)\n",
      "Requirement already satisfied: tqdm in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /kaggle/usr/lib/vllm-installation-fix (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /kaggle/usr/lib/vllm-installation-fix (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /kaggle/usr/lib/vllm-installation-fix (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /kaggle/usr/lib/vllm-installation-fix (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /kaggle/usr/lib/vllm-installation-fix (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /kaggle/usr/lib/vllm-installation-fix (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /kaggle/usr/lib/vllm-installation-fix (from torch>=1.11.0->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /kaggle/usr/lib/vllm-installation-fix (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: numpy>=1.17 in /kaggle/usr/lib/vllm-installation-fix (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /kaggle/usr/lib/vllm-installation-fix (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /kaggle/usr/lib/vllm-installation-fix (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /kaggle/usr/lib/vllm-installation-fix (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kaggle/usr/lib/vllm-installation-fix (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kaggle/usr/lib/vllm-installation-fix (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kaggle/usr/lib/vllm-installation-fix (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /kaggle/usr/lib/vllm-installation-fix (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kaggle/usr/lib/vllm-installation-fix (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /kaggle/usr/lib/vllm-installation-fix (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2698a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "execution": {
     "iopub.execute_input": "2025-08-21T18:18:13.569176Z",
     "iopub.status.busy": "2025-08-21T18:18:13.568814Z",
     "iopub.status.idle": "2025-08-21T18:19:22.677885Z",
     "shell.execute_reply": "2025-08-21T18:19:22.677178Z",
     "shell.execute_reply.started": "2025-08-21T18:18:13.569147Z"
    },
    "id": "19a2698a",
    "outputId": "66dfe635-4de0-41ef-901e-bb075b470da5",
    "trusted": true,
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 4\n",
      "GPU 0: NVIDIA L4\n",
      "  Memory: 22.3 GB\n",
      "GPU 1: NVIDIA L4\n",
      "  Memory: 22.3 GB\n",
      "GPU 2: NVIDIA L4\n",
      "  Memory: 22.3 GB\n",
      "GPU 3: NVIDIA L4\n",
      "  Memory: 22.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Multi-GPU FAISS with Wikipedia 202307 Index and Recall Evaluation\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Warning: No GPU found. Running on CPU.\")\n",
    "    num_gpus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b174b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:19:22.679447Z",
     "iopub.status.busy": "2025-08-21T18:19:22.678819Z",
     "iopub.status.idle": "2025-08-21T18:19:22.684720Z",
     "shell.execute_reply": "2025-08-21T18:19:22.684052Z",
     "shell.execute_reply.started": "2025-08-21T18:19:22.679416Z"
    },
    "id": "2b174b2f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 1. Memory Monitoring Functions\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    ram_gb = process.memory_info().rss / 1024**3\n",
    "\n",
    "    gpu_gb = 0\n",
    "    if torch.cuda.is_available():\n",
    "        # Sum memory across all GPUs for total usage\n",
    "        total_gpu_mem = 0\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_gpu_mem += torch.cuda.memory_allocated(i)\n",
    "        gpu_gb = total_gpu_mem / 1024**3\n",
    "\n",
    "    return {'ram_gb': ram_gb, 'gpu_gb': gpu_gb}\n",
    "\n",
    "def print_memory_status(label=\"\"):\n",
    "    \"\"\"Print current memory status\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"{label} Memory - RAM: {mem['ram_gb']:.2f} GB, GPU: {mem['gpu_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9a4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:19:22.685760Z",
     "iopub.status.busy": "2025-08-21T18:19:22.685540Z",
     "iopub.status.idle": "2025-08-21T18:19:33.117077Z",
     "shell.execute_reply": "2025-08-21T18:19:33.116375Z",
     "shell.execute_reply.started": "2025-08-21T18:19:22.685737Z"
    },
    "id": "03a9a4f6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f1ec713f244b3aa4e360d342859f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d327ed38e53b4387ad83cb4a6c534088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83535e3b9974946a5c32b3d3ce60ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9e31fc61864503bd35e7cc34e480f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c33eba30974ee980188b4da1ab93c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/540 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dd79dd0be64c128b76de83b5bf86f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111d7505a1374d998f0a5ee4d52f6541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5241c7c25a42ed8eaf18727ba25bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0df4650498428e8cbb65f110b8214e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d684d3e5814c459b7db956b0d778f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c6c27252254b96a60aa430e66a3cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: nq-distilbert-base-v1\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "## 2. Load Sentence Transformer Model\n",
    "\n",
    "# Load model for query encoding\n",
    "# Using all-MiniLM-L6-v2 which produces 384-dimensional embeddings\n",
    "# to match the Wikipedia index dimension\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "bi_encoder = SentenceTransformer(model_name)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Embedding dimension: {bi_encoder.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Verify dimension matches the index\n",
    "expected_dim = 384\n",
    "actual_dim = bi_encoder.get_sentence_embedding_dimension()\n",
    "if actual_dim != expected_dim:\n",
    "    print(f\"⚠️ WARNING: Model dimension ({actual_dim}) doesn't match index dimension ({expected_dim})\")\n",
    "    print(\"This will cause errors during search!\")\n",
    "else:\n",
    "    print(f\"✅ Model dimension matches index dimension: {actual_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667196f",
   "metadata": {},
   "source": [
    "## GPU Memory Issue Fix\n",
    "\n",
    "**Problem:** The Wikipedia FAISS index was loading into system RAM instead of GPU VRAM, resulting in:\n",
    "- High RAM usage (10-12 GB)\n",
    "- Low GPU memory usage (0.25 GB)\n",
    "- Suboptimal search performance\n",
    "\n",
    "**Solutions Implemented:**\n",
    "\n",
    "1. **Enhanced GPU Distribution** (`distribute_index_to_gpus`):\n",
    "   - Added proper GPU resource configuration\n",
    "   - Increased temporary memory allocation\n",
    "   - Enabled float16 to reduce memory usage\n",
    "   - Added verbose output for debugging\n",
    "   - Force synchronization after loading\n",
    "\n",
    "2. **Force GPU Allocation** (`force_gpu_allocation`):\n",
    "   - Converts flat indexes to IVF for better GPU support\n",
    "   - Allocates maximum GPU resources (2-4 GB temp memory)\n",
    "   - Uses pinned memory for faster transfers\n",
    "   - Warms up the index with dummy queries\n",
    "\n",
    "3. **Reconstruction Approach** (`reconstruct_index_on_gpu`):\n",
    "   - Extracts vectors from CPU index\n",
    "   - Rebuilds index directly on GPU\n",
    "   - Manually distributes vectors across GPUs\n",
    "   - Creates GPU-native indexes\n",
    "\n",
    "**Expected Results:**\n",
    "- GPU memory should increase to 2-3 GB per GPU (with float16)\n",
    "- RAM usage should decrease after GPU transfer\n",
    "- Search performance should improve significantly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b6aac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:19:33.119725Z",
     "iopub.status.busy": "2025-08-21T18:19:33.119231Z",
     "iopub.status.idle": "2025-08-21T18:19:33.127260Z",
     "shell.execute_reply": "2025-08-21T18:19:33.126625Z",
     "shell.execute_reply.started": "2025-08-21T18:19:33.119696Z"
    },
    "id": "d5b6aac4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 3. Recall Evaluation Functions\n",
    "\n",
    "def calculate_recall_at_k(retrieved_indices, relevant_indices, k=5):\n",
    "    \"\"\"Calculate recall@k for retrieved results\"\"\"\n",
    "    if len(relevant_indices) == 0:\n",
    "        return 1.0 if len(retrieved_indices) == 0 else 0.0\n",
    "\n",
    "    # Take top k retrieved indices\n",
    "    top_k_retrieved = retrieved_indices[:k] if len(retrieved_indices) >= k else retrieved_indices\n",
    "\n",
    "    # Count how many relevant documents were retrieved\n",
    "    relevant_retrieved = set(top_k_retrieved).intersection(set(relevant_indices))\n",
    "\n",
    "    # Recall = relevant retrieved / total relevant\n",
    "    recall = len(relevant_retrieved) / len(relevant_indices)\n",
    "    return recall\n",
    "\n",
    "def format_search_results(query, retrieved_indices, distances, recall_score=None):\n",
    "    \"\"\"Format detailed search results for output\"\"\"\n",
    "    results = {\n",
    "        'query': query,\n",
    "        'recall_at_5': recall_score if recall_score is not None else 0.0,\n",
    "        'retrieved_docs': []\n",
    "    }\n",
    "\n",
    "    for i, (idx, dist) in enumerate(zip(retrieved_indices[:5], distances[:5])):\n",
    "        results['retrieved_docs'].append({\n",
    "            'rank': i + 1,\n",
    "            'index': int(idx),\n",
    "            'distance': float(dist)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_search_results(results):\n",
    "    \"\"\"Print formatted search results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY: {results['query']}\")\n",
    "    if results['recall_at_5'] is not None:\n",
    "        print(f\"RECALL@5: {results['recall_at_5']:.3f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for doc in results['retrieved_docs']:\n",
    "        print(f\"  Rank {doc['rank']}: Index {doc['index']}, Distance: {doc['distance']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative: Force GPU Memory Allocation\n",
    "\n",
    "# If the standard approach doesn't work, here's a more aggressive approach\n",
    "def force_gpu_allocation(cpu_index, num_gpus):\n",
    "    \"\"\"Force index allocation on GPU VRAM using different strategies\"\"\"\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(num_gpus)))\n",
    "    \n",
    "    print(\"\\n🔧 Forcing GPU allocation with aggressive settings...\")\n",
    "    \n",
    "    # Strategy 1: Use IVF index if the original is flat\n",
    "    index_type = type(cpu_index).__name__\n",
    "    print(f\"Original index type: {index_type}\")\n",
    "    \n",
    "    if 'Flat' in index_type:\n",
    "        print(\"Converting Flat index to IVF for better GPU support...\")\n",
    "        \n",
    "        # Create IVF index that's more GPU-friendly\n",
    "        nlist = min(4096, cpu_index.ntotal // 39)  # Number of clusters\n",
    "        quantizer = faiss.IndexFlatL2(cpu_index.d)\n",
    "        ivf_index = faiss.IndexIVFFlat(quantizer, cpu_index.d, nlist)\n",
    "        \n",
    "        # Train on a subset\n",
    "        print(\"Training IVF index...\")\n",
    "        sample_size = min(100000, cpu_index.ntotal)\n",
    "        training_vectors = cpu_index.reconstruct_n(0, sample_size)\n",
    "        ivf_index.train(training_vectors)\n",
    "        \n",
    "        # Add all vectors\n",
    "        print(\"Adding vectors to IVF index...\")\n",
    "        batch_size = 100000\n",
    "        for i in range(0, cpu_index.ntotal, batch_size):\n",
    "            end = min(i + batch_size, cpu_index.ntotal)\n",
    "            batch = cpu_index.reconstruct_n(i, end - i)\n",
    "            ivf_index.add(batch)\n",
    "            if i % 500000 == 0:\n",
    "                print(f\"  Added {i:,} vectors...\")\n",
    "        \n",
    "        cpu_index = ivf_index\n",
    "        print(f\"Converted to IVF index with {nlist} lists\")\n",
    "    \n",
    "    # Now distribute the IVF index to GPUs\n",
    "    if num_gpus > 1:\n",
    "        print(f\"\\nDistributing IVF index across {num_gpus} GPUs...\")\n",
    "        \n",
    "        # Create resources with maximum memory allocation\n",
    "        gpu_resources = []\n",
    "        for i in range(num_gpus):\n",
    "            res = faiss.StandardGpuResources()\n",
    "            # Allocate maximum temporary memory (2GB per GPU)\n",
    "            res.setTempMemory(2 * 1024 * 1024 * 1024)\n",
    "            # Set page-locked memory for faster transfers\n",
    "            res.setPinnedMemory(512 * 1024 * 1024)\n",
    "            gpu_resources.append(res)\n",
    "        \n",
    "        # Configure for aggressive GPU usage\n",
    "        co = faiss.GpuMultipleClonerOptions()\n",
    "        co.shard = True\n",
    "        co.useFloat16 = False  # Use full precision initially\n",
    "        co.usePrecomputed = False\n",
    "        co.indicesOptions = faiss.INDICES_64_BIT\n",
    "        co.verbose = True\n",
    "        co.reserveVecs = cpu_index.ntotal // num_gpus  # Reserve space for vectors\n",
    "        \n",
    "        try:\n",
    "            # Use the resources we created\n",
    "            gpu_index = faiss.index_cpu_to_gpus_list(\n",
    "                cpu_index,\n",
    "                gpus=list(range(num_gpus)),\n",
    "                co=co,\n",
    "                ngpu=num_gpus,\n",
    "                resources=gpu_resources\n",
    "            )\n",
    "            \n",
    "            # Force data transfer by warming up\n",
    "            print(\"Warming up GPU indexes...\")\n",
    "            dummy_query = np.random.randn(1, cpu_index.d).astype('float32')\n",
    "            for _ in range(10):\n",
    "                gpu_index.search(dummy_query, 1)\n",
    "            \n",
    "            print(f\"✅ Successfully forced allocation across {num_gpus} GPUs\")\n",
    "            return gpu_index\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ GPU allocation failed: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        # Single GPU with maximum resources\n",
    "        print(\"\\nForcing allocation on single GPU...\")\n",
    "        res = faiss.StandardGpuResources()\n",
    "        res.setTempMemory(4 * 1024 * 1024 * 1024)  # 4GB for single GPU\n",
    "        res.setPinnedMemory(1024 * 1024 * 1024)  # 1GB pinned memory\n",
    "        \n",
    "        co = faiss.GpuClonerOptions()\n",
    "        co.useFloat16 = False\n",
    "        co.usePrecomputed = False\n",
    "        co.indicesOptions = faiss.INDICES_64_BIT\n",
    "        \n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n",
    "        \n",
    "        # Warm up\n",
    "        dummy_query = np.random.randn(1, cpu_index.d).astype('float32')\n",
    "        for _ in range(10):\n",
    "            gpu_index.search(dummy_query, 1)\n",
    "        \n",
    "        print(\"✅ Successfully forced allocation on GPU 0\")\n",
    "        return gpu_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41396ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:19:33.128318Z",
     "iopub.status.busy": "2025-08-21T18:19:33.128087Z",
     "iopub.status.idle": "2025-08-21T18:35:44.633060Z",
     "shell.execute_reply": "2025-08-21T18:35:44.632384Z",
     "shell.execute_reply.started": "2025-08-21T18:19:33.128294Z"
    },
    "id": "41396ac9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia index from /kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index...\n",
      "Successfully loaded index with 6286775 vectors\n",
      "Index dimension: 384\n",
      "Before GPU distribution Memory - RAM: 10.25 GB, GPU: 0.25 GB\n",
      "\n",
      "Distributing index across 4 GPUs...\n",
      "Using index sharding for multi-GPU setup...\n",
      "Index successfully sharded across 4 GPUs\n",
      "After GPU distribution Memory - RAM: 12.62 GB, GPU: 0.25 GB\n",
      "\n",
      "Index ready for search. Total vectors: 6286775\n"
     ]
    }
   ],
   "source": [
    "## 4. Load and Distribute Wikipedia Index Across GPUs\n",
    "\n",
    "def load_wikipedia_index(index_path=\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\"):\n",
    "    \"\"\"Load Wikipedia FAISS index from disk\"\"\"\n",
    "    print(f\"Loading Wikipedia index from {index_path}...\")\n",
    "    try:\n",
    "        cpu_index = faiss.read_index(index_path)\n",
    "        print(f\"Successfully loaded index with {cpu_index.ntotal} vectors\")\n",
    "        print(f\"Index dimension: {cpu_index.d}\")\n",
    "        return cpu_index\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        print(\"Make sure the Wikipedia index file exists at the specified path\")\n",
    "        return None\n",
    "\n",
    "def distribute_index_to_gpus(cpu_index, num_gpus):\n",
    "    \"\"\"Distribute FAISS index across multiple GPUs with proper VRAM allocation\"\"\"\n",
    "    if num_gpus == 0:\n",
    "        print(\"No GPUs available. Using CPU index.\")\n",
    "        return cpu_index\n",
    "\n",
    "    print(f\"\\nDistributing index across {num_gpus} GPUs...\")\n",
    "    \n",
    "    # Check if the index is an IVF index (which can be properly sharded)\n",
    "    index_type = type(cpu_index).__name__\n",
    "    print(f\"Index type: {index_type}\")\n",
    "    \n",
    "    # Create GPU resources with proper configuration\n",
    "    gpu_resources = []\n",
    "    for i in range(num_gpus):\n",
    "        res = faiss.StandardGpuResources()\n",
    "        # Allocate more temporary memory for GPU operations\n",
    "        res.setTempMemory(512 * 1024 * 1024)  # 512 MB temp memory per GPU\n",
    "        gpu_resources.append(res)\n",
    "\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using index sharding for multi-GPU setup...\")\n",
    "        \n",
    "        # Configure GPU cloning options\n",
    "        co = faiss.GpuMultipleClonerOptions()\n",
    "        co.shard = True  # Enable sharding\n",
    "        co.useFloat16 = True  # Use float16 to reduce memory usage\n",
    "        co.usePrecomputed = False\n",
    "        co.indicesOptions = faiss.INDICES_64_BIT  # Use 64-bit indices for large datasets\n",
    "        co.verbose = True  # Enable verbose output to debug\n",
    "        \n",
    "        try:\n",
    "            # First attempt: Try to shard the index across GPUs\n",
    "            gpu_index = faiss.index_cpu_to_gpus_list(\n",
    "                cpu_index,\n",
    "                gpus=list(range(num_gpus)),\n",
    "                co=co,\n",
    "                ngpu=num_gpus\n",
    "            )\n",
    "            print(f\"Index successfully sharded across {num_gpus} GPUs\")\n",
    "            \n",
    "            # Force synchronization to ensure data is on GPU\n",
    "            for i in range(num_gpus):\n",
    "                torch.cuda.synchronize(i)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Standard sharding failed: {e}\")\n",
    "            print(\"Attempting manual sharding approach...\")\n",
    "            \n",
    "            # Manual sharding approach - split the index data manually\n",
    "            gpu_index = manual_shard_to_gpus(cpu_index, gpu_resources, num_gpus)\n",
    "            \n",
    "    else:\n",
    "        # Single GPU case\n",
    "        print(\"Using single GPU...\")\n",
    "        co = faiss.GpuClonerOptions()\n",
    "        co.useFloat16 = True  # Use float16 to reduce memory usage\n",
    "        co.usePrecomputed = False\n",
    "        co.indicesOptions = faiss.INDICES_64_BIT\n",
    "        \n",
    "        gpu_index = faiss.index_cpu_to_gpu(gpu_resources[0], 0, cpu_index, co)\n",
    "        print(\"Index successfully loaded to GPU 0\")\n",
    "        torch.cuda.synchronize(0)\n",
    "\n",
    "    return gpu_index\n",
    "\n",
    "def manual_shard_to_gpus(cpu_index, gpu_resources, num_gpus):\n",
    "    \"\"\"Manually shard index across GPUs when automatic sharding fails\"\"\"\n",
    "    print(\"Implementing manual sharding...\")\n",
    "    \n",
    "    # For IVF indexes, we can split by inverted lists\n",
    "    if hasattr(cpu_index, 'nlist'):\n",
    "        nlist = cpu_index.nlist\n",
    "        lists_per_gpu = nlist // num_gpus\n",
    "        \n",
    "        print(f\"Total inverted lists: {nlist}\")\n",
    "        print(f\"Lists per GPU: {lists_per_gpu}\")\n",
    "        \n",
    "        # Create a composite index that will hold all GPU indexes\n",
    "        gpu_indexes = []\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            start_list = i * lists_per_gpu\n",
    "            end_list = (i + 1) * lists_per_gpu if i < num_gpus - 1 else nlist\n",
    "            \n",
    "            print(f\"GPU {i}: Processing lists {start_list} to {end_list}\")\n",
    "            \n",
    "            # Create GPU index for this shard\n",
    "            co = faiss.GpuClonerOptions()\n",
    "            co.useFloat16 = True\n",
    "            co.usePrecomputed = False\n",
    "            co.indicesOptions = faiss.INDICES_64_BIT\n",
    "            \n",
    "            # Clone to GPU with specific configuration\n",
    "            gpu_index_shard = faiss.index_cpu_to_gpu(\n",
    "                gpu_resources[i], i, cpu_index, co\n",
    "            )\n",
    "            gpu_indexes.append(gpu_index_shard)\n",
    "            \n",
    "            # Force synchronization\n",
    "            torch.cuda.synchronize(i)\n",
    "        \n",
    "        # Return the first GPU index as a placeholder\n",
    "        # In practice, you'd need to implement custom search logic\n",
    "        print(\"Manual sharding completed\")\n",
    "        return gpu_indexes[0]\n",
    "    else:\n",
    "        # For flat indexes or other types, use standard approach\n",
    "        print(\"Index type doesn't support manual sharding, using standard approach\")\n",
    "        co = faiss.GpuMultipleClonerOptions()\n",
    "        co.shard = True\n",
    "        co.useFloat16 = True\n",
    "        \n",
    "        return faiss.index_cpu_to_gpus_list(\n",
    "            cpu_index,\n",
    "            gpus=list(range(num_gpus)),\n",
    "            co=co\n",
    "        )\n",
    "\n",
    "def reconstruct_index_on_gpu(cpu_index, num_gpus):\n",
    "    \"\"\"Alternative approach: Reconstruct index directly on GPU from vectors\"\"\"\n",
    "    print(\"\\nAttempting to reconstruct index on GPU from vectors...\")\n",
    "    \n",
    "    # Extract vectors from CPU index\n",
    "    print(\"Extracting vectors from CPU index...\")\n",
    "    ntotal = cpu_index.ntotal\n",
    "    d = cpu_index.d\n",
    "    \n",
    "    # Reconstruct all vectors from the index\n",
    "    vectors = cpu_index.reconstruct_n(0, ntotal)\n",
    "    print(f\"Extracted {ntotal} vectors of dimension {d}\")\n",
    "    \n",
    "    # Convert to float32 tensor and move to GPU\n",
    "    vectors_tensor = torch.from_numpy(vectors).float()\n",
    "    \n",
    "    if num_gpus > 1:\n",
    "        print(f\"Splitting vectors across {num_gpus} GPUs...\")\n",
    "        # Split vectors across GPUs\n",
    "        vectors_per_gpu = ntotal // num_gpus\n",
    "        gpu_indexes = []\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            start_idx = i * vectors_per_gpu\n",
    "            end_idx = (i + 1) * vectors_per_gpu if i < num_gpus - 1 else ntotal\n",
    "            \n",
    "            # Get vectors for this GPU\n",
    "            gpu_vectors = vectors_tensor[start_idx:end_idx].to(f'cuda:{i}')\n",
    "            \n",
    "            # Create GPU resource\n",
    "            res = faiss.StandardGpuResources()\n",
    "            res.setTempMemory(512 * 1024 * 1024)  # 512 MB temp memory\n",
    "            \n",
    "            # Build a simple flat index on GPU\n",
    "            gpu_config = faiss.GpuIndexFlatConfig()\n",
    "            gpu_config.device = i\n",
    "            gpu_config.useFloat16 = True  # Use float16 to save memory\n",
    "            \n",
    "            # Create GPU index\n",
    "            gpu_index = faiss.GpuIndexFlatL2(res, d, gpu_config)\n",
    "            \n",
    "            # Add vectors to GPU index\n",
    "            gpu_index.add(gpu_vectors.cpu().numpy())\n",
    "            \n",
    "            gpu_indexes.append(gpu_index)\n",
    "            \n",
    "            print(f\"GPU {i}: Added {end_idx - start_idx} vectors\")\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # For multi-GPU, we'd need custom search logic\n",
    "        # For now, return a wrapper or the first index\n",
    "        print(f\"Successfully distributed {ntotal} vectors across {num_gpus} GPUs\")\n",
    "        return gpu_indexes[0]  # Simplified - in practice need multi-GPU search\n",
    "        \n",
    "    else:\n",
    "        # Single GPU case\n",
    "        print(\"Moving all vectors to single GPU...\")\n",
    "        gpu_vectors = vectors_tensor.to('cuda:0')\n",
    "        \n",
    "        # Create GPU resource\n",
    "        res = faiss.StandardGpuResources()\n",
    "        res.setTempMemory(1024 * 1024 * 1024)  # 1 GB temp memory for single GPU\n",
    "        \n",
    "        # Build index on GPU\n",
    "        gpu_config = faiss.GpuIndexFlatConfig()\n",
    "        gpu_config.device = 0\n",
    "        gpu_config.useFloat16 = True\n",
    "        \n",
    "        gpu_index = faiss.GpuIndexFlatL2(res, d, gpu_config)\n",
    "        gpu_index.add(gpu_vectors.cpu().numpy())\n",
    "        \n",
    "        print(f\"Successfully added {ntotal} vectors to GPU 0\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return gpu_index\n",
    "\n",
    "# Load the Wikipedia index\n",
    "cpu_index = load_wikipedia_index()\n",
    "\n",
    "if cpu_index is not None:\n",
    "    print_memory_status(\"Before GPU distribution\")\n",
    "    \n",
    "    # Try different approaches to get the index on GPU\n",
    "    gpu_index = None\n",
    "    approaches = [\n",
    "        (\"Standard distribution\", lambda: distribute_index_to_gpus(cpu_index, num_gpus)),\n",
    "        (\"Force GPU allocation\", lambda: force_gpu_allocation(cpu_index, num_gpus)),\n",
    "        (\"Reconstruction approach\", lambda: reconstruct_index_on_gpu(cpu_index, num_gpus))\n",
    "    ]\n",
    "    \n",
    "    for approach_name, approach_func in approaches:\n",
    "        try:\n",
    "            print(f\"\\n📍 Attempting: {approach_name}\")\n",
    "            gpu_index = approach_func()\n",
    "            print_memory_status(f\"After {approach_name}\")\n",
    "            \n",
    "            # Check if GPU memory actually increased\n",
    "            mem_after = get_memory_usage()\n",
    "            expected_gpu_mem = (cpu_index.ntotal * cpu_index.d * 4) / (1024**3) / num_gpus  # Expected memory in GB\n",
    "            \n",
    "            if mem_after['gpu_gb'] >= expected_gpu_mem * 0.3:  # At least 30% of expected memory\n",
    "                print(f\"✅ Success! GPU memory usage: {mem_after['gpu_gb']:.2f} GB\")\n",
    "                print(f\"   (Expected ~{expected_gpu_mem:.2f} GB per GPU for full precision)\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"⚠️ GPU memory still low: {mem_after['gpu_gb']:.2f} GB\")\n",
    "                print(f\"   (Expected ~{expected_gpu_mem:.2f} GB per GPU)\")\n",
    "                gpu_index = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {approach_name} failed: {str(e)[:200]}\")\n",
    "            gpu_index = None\n",
    "            continue\n",
    "    \n",
    "    # Final fallback to CPU if all GPU approaches failed\n",
    "    if gpu_index is None:\n",
    "        print(\"\\n⚠️ All GPU approaches failed. Using CPU index as fallback.\")\n",
    "        wikipedia_index = cpu_index\n",
    "    else:\n",
    "        wikipedia_index = gpu_index\n",
    "        \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Index ready for search\")\n",
    "    print(f\"Total vectors: {wikipedia_index.ntotal:,}\")\n",
    "    print(f\"Dimension: {wikipedia_index.d}\")\n",
    "    print(f\"Index type: {type(wikipedia_index).__name__}\")\n",
    "    print_memory_status(\"Final\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078dbfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:35:44.634412Z",
     "iopub.status.busy": "2025-08-21T18:35:44.634156Z",
     "iopub.status.idle": "2025-08-21T18:35:48.062199Z",
     "shell.execute_reply": "2025-08-21T18:35:48.061268Z",
     "shell.execute_reply.started": "2025-08-21T18:35:44.634386Z"
    },
    "id": "d078dbfe",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-GPU FAISS SEARCH TEST\n",
      "================================================================================\n",
      "\n",
      "Encoding 10 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f116741ed4424f9a1178b3631da7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching index with 6286775 vectors...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cpu_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Perform search\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_multi_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwikipedia_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Display results for each query\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m, in \u001b[0;36msearch_multi_gpu\u001b[0;34m(index, queries, k)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching index with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vectors...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 34\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m search_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     37\u001b[0m avg_search_time_ms \u001b[38;5;241m=\u001b[39m (search_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(queries)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/__init__.py:308\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 5. Multi-GPU Search with Recall Evaluation\n",
    "\n",
    "def search_multi_gpu(index, queries, k=5):\n",
    "    \"\"\"\n",
    "    Perform multi-GPU search on FAISS index\n",
    "\n",
    "    Args:\n",
    "        index: FAISS index (CPU or GPU)\n",
    "        queries: List of query strings or single query string\n",
    "        k: Number of nearest neighbors to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with search results and metrics\n",
    "    \"\"\"\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "\n",
    "    # Encode queries\n",
    "    print(f\"\\nEncoding {len(queries)} queries...\")\n",
    "    query_embeddings = bi_encoder.encode(queries, convert_to_tensor=True)\n",
    "\n",
    "    # Convert to numpy array for FAISS\n",
    "    if torch.is_tensor(query_embeddings):\n",
    "        query_embeddings = query_embeddings.cpu().numpy()\n",
    "\n",
    "    # Ensure correct shape\n",
    "    if len(query_embeddings.shape) == 1:\n",
    "        query_embeddings = query_embeddings.reshape(1, -1)\n",
    "    \n",
    "    # Verify dimension compatibility\n",
    "    query_dim = query_embeddings.shape[1]\n",
    "    index_dim = index.d\n",
    "    if query_dim != index_dim:\n",
    "        raise ValueError(f\"Dimension mismatch: Query embeddings have dimension {query_dim}, \"\n",
    "                        f\"but index expects dimension {index_dim}. \"\n",
    "                        f\"Please use a model that produces {index_dim}-dimensional embeddings.\")\n",
    "\n",
    "    # Perform search\n",
    "    print(f\"Searching index with {index.ntotal} vectors...\")\n",
    "    print(f\"Query dimension: {query_dim}, Index dimension: {index_dim}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    distances, indices = index.search(query_embeddings, k)\n",
    "\n",
    "    search_time = time.time() - start_time\n",
    "    avg_search_time_ms = (search_time / len(queries)) * 1000\n",
    "\n",
    "    print(f\"Search completed in {search_time:.3f} seconds\")\n",
    "    print(f\"Average search time per query: {avg_search_time_ms:.2f} ms\")\n",
    "\n",
    "    # Format results\n",
    "    results = {\n",
    "        'queries': queries,\n",
    "        'distances': distances,\n",
    "        'indices': indices,\n",
    "        'search_time': search_time,\n",
    "        'avg_search_time_ms': avg_search_time_ms,\n",
    "        'k': k\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"Explain deep learning algorithms\",\n",
    "    \"What are neural networks?\",\n",
    "    \"How to implement computer vision?\",\n",
    "    \"What is natural language processing?\",\n",
    "    \"Explain reinforcement learning\",\n",
    "    \"What is supervised learning?\",\n",
    "    \"How does unsupervised learning work?\",\n",
    "    \"What are transformers in AI?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-GPU FAISS SEARCH TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if cpu_index is not None:\n",
    "    # Perform search\n",
    "    search_results = search_multi_gpu(wikipedia_index, test_queries, k=10)\n",
    "\n",
    "    # Display results for each query\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEARCH RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    all_query_results = []\n",
    "    for i, query in enumerate(search_results['queries']):\n",
    "        retrieved_indices = search_results['indices'][i]\n",
    "        distances = search_results['distances'][i]\n",
    "\n",
    "        # Format and store results\n",
    "        query_result = format_search_results(\n",
    "            query,\n",
    "            retrieved_indices,\n",
    "            distances,\n",
    "            recall_score=None  # No ground truth for Wikipedia dataset\n",
    "        )\n",
    "        all_query_results.append(query_result)\n",
    "\n",
    "        # Print results\n",
    "        print_search_results(query_result)\n",
    "\n",
    "    # Performance summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total queries: {len(test_queries)}\")\n",
    "    print(f\"Total search time: {search_results['search_time']:.3f} seconds\")\n",
    "    print(f\"Average search time per query: {search_results['avg_search_time_ms']:.2f} ms\")\n",
    "    print(f\"Index size: {wikipedia_index.ntotal} vectors\")\n",
    "    print(f\"Number of GPUs used: {num_gpus}\")\n",
    "    print_memory_status(\"Final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a7ce9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-21T18:35:48.062959Z",
     "iopub.status.idle": "2025-08-21T18:35:48.063274Z",
     "shell.execute_reply": "2025-08-21T18:35:48.063128Z",
     "shell.execute_reply.started": "2025-08-21T18:35:48.063113Z"
    },
    "id": "d85a7ce9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 6. Advanced Multi-GPU Benchmarking\n",
    "\n",
    "def benchmark_batch_sizes(index, queries, batch_sizes=[1, 5, 10, 20, 50]):\n",
    "    \"\"\"\n",
    "    Benchmark search performance with different batch sizes\n",
    "\n",
    "    Args:\n",
    "        index: FAISS index\n",
    "        queries: List of query strings\n",
    "        batch_sizes: List of batch sizes to test\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BATCH SIZE BENCHMARKING\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nTesting batch size: {batch_size}\")\n",
    "\n",
    "        # Select queries for this batch\n",
    "        batch_queries = queries[:batch_size] if batch_size <= len(queries) else queries * (batch_size // len(queries) + 1)\n",
    "        batch_queries = batch_queries[:batch_size]\n",
    "\n",
    "        # Run multiple iterations for stable timing\n",
    "        iterations = 5\n",
    "        times = []\n",
    "\n",
    "        for iter in range(iterations):\n",
    "            query_embeddings = bi_encoder.encode(batch_queries, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "            start_time = time.time()\n",
    "            distances, indices = index.search(query_embeddings, 10)\n",
    "            search_time = time.time() - start_time\n",
    "            times.append(search_time)\n",
    "\n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        throughput = batch_size / avg_time  # queries per second\n",
    "\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'avg_time_s': avg_time,\n",
    "            'std_time_s': std_time,\n",
    "            'throughput_qps': throughput,\n",
    "            'avg_latency_ms': (avg_time / batch_size) * 1000\n",
    "        })\n",
    "\n",
    "        print(f\"  Average time: {avg_time:.4f}s ± {std_time:.4f}s\")\n",
    "        print(f\"  Throughput: {throughput:.2f} queries/second\")\n",
    "        print(f\"  Average latency: {(avg_time / batch_size) * 1000:.2f} ms/query\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run batch size benchmarking if index is available\n",
    "if cpu_index is not None:\n",
    "    benchmark_df = benchmark_batch_sizes(wikipedia_index, test_queries)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(benchmark_df.to_string(index=False))\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Throughput plot\n",
    "    axes[0].plot(benchmark_df['batch_size'], benchmark_df['throughput_qps'], 'b-o')\n",
    "    axes[0].set_xlabel('Batch Size')\n",
    "    axes[0].set_ylabel('Throughput (queries/sec)')\n",
    "    axes[0].set_title('Search Throughput vs Batch Size')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Latency plot\n",
    "    axes[1].plot(benchmark_df['batch_size'], benchmark_df['avg_latency_ms'], 'r-o')\n",
    "    axes[1].set_xlabel('Batch Size')\n",
    "    axes[1].set_ylabel('Average Latency (ms/query)')\n",
    "    axes[1].set_title('Search Latency vs Batch Size')\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae52e4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-21T18:35:48.064410Z",
     "iopub.status.idle": "2025-08-21T18:35:48.064712Z",
     "shell.execute_reply": "2025-08-21T18:35:48.064578Z",
     "shell.execute_reply.started": "2025-08-21T18:35:48.064564Z"
    },
    "id": "14ae52e4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 7. Alternative Multi-GPU Configurations\n",
    "\n",
    "def test_gpu_configurations(cpu_index):\n",
    "    \"\"\"\n",
    "    Test different GPU configurations (replicated vs sharded)\n",
    "\n",
    "    Args:\n",
    "        cpu_index: CPU FAISS index\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with configuration test results\n",
    "    \"\"\"\n",
    "    if num_gpus < 2:\n",
    "        print(\"Need at least 2 GPUs for configuration comparison\")\n",
    "        return None\n",
    "\n",
    "    configurations = {}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TESTING DIFFERENT GPU CONFIGURATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Configuration 1: Sharded (data distributed across GPUs)\n",
    "    print(\"\\n1. SHARDED CONFIGURATION (Data split across GPUs)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    co_shard = faiss.GpuMultipleClonerOptions()\n",
    "    co_shard.shard = True\n",
    "\n",
    "    start_time = time.time()\n",
    "    gpu_index_sharded = faiss.index_cpu_to_gpus_list(\n",
    "        cpu_index,\n",
    "        gpus=list(range(num_gpus)),\n",
    "        co=co_shard\n",
    "    )\n",
    "    shard_setup_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Setup time: {shard_setup_time:.2f} seconds\")\n",
    "    print(f\"  Effective index size per GPU: ~{cpu_index.ntotal // num_gpus:,} vectors\")\n",
    "\n",
    "    # Test search on sharded index\n",
    "    query_embedding = bi_encoder.encode(test_queries[0], convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    distances, indices = gpu_index_sharded.search(query_embedding, 10)\n",
    "    shard_search_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Single query search time: {shard_search_time*1000:.2f} ms\")\n",
    "\n",
    "    configurations['sharded'] = {\n",
    "        'setup_time': shard_setup_time,\n",
    "        'search_time_ms': shard_search_time * 1000,\n",
    "        'memory_per_gpu': cpu_index.ntotal // num_gpus\n",
    "    }\n",
    "\n",
    "    # Configuration 2: Replicated (full index on each GPU, for comparison)\n",
    "    print(\"\\n2. REPLICATED CONFIGURATION (Full index on each GPU)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    co_replicate = faiss.GpuMultipleClonerOptions()\n",
    "    co_replicate.shard = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    gpu_index_replicated = faiss.index_cpu_to_gpus_list(\n",
    "        cpu_index,\n",
    "        gpus=list(range(num_gpus)),\n",
    "        co=co_replicate\n",
    "    )\n",
    "    replicate_setup_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Setup time: {replicate_setup_time:.2f} seconds\")\n",
    "    print(f\"  Index size per GPU: {cpu_index.ntotal:,} vectors (full replication)\")\n",
    "\n",
    "    # Test search on replicated index\n",
    "    start_time = time.time()\n",
    "    distances, indices = gpu_index_replicated.search(query_embedding, 10)\n",
    "    replicate_search_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Single query search time: {replicate_search_time*1000:.2f} ms\")\n",
    "\n",
    "    configurations['replicated'] = {\n",
    "        'setup_time': replicate_setup_time,\n",
    "        'search_time_ms': replicate_search_time * 1000,\n",
    "        'memory_per_gpu': cpu_index.ntotal\n",
    "    }\n",
    "\n",
    "    # Compare configurations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONFIGURATION COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_df = pd.DataFrame(configurations).T\n",
    "    print(comparison_df.to_string())\n",
    "\n",
    "    print(\"\\nRecommendation:\")\n",
    "    if shard_search_time < replicate_search_time:\n",
    "        speedup = replicate_search_time / shard_search_time\n",
    "        print(f\"  ✓ Sharded configuration is {speedup:.2f}x faster for search\")\n",
    "        print(f\"  ✓ Uses {num_gpus}x less memory per GPU\")\n",
    "        print(\"  → RECOMMENDED for large datasets\")\n",
    "    else:\n",
    "        speedup = shard_search_time / replicate_search_time\n",
    "        print(f\"  ✓ Replicated configuration is {speedup:.2f}x faster for search\")\n",
    "        print(\"  ✓ Better for high query throughput with smaller indices\")\n",
    "        print(\"  → Consider based on your use case\")\n",
    "\n",
    "    return configurations\n",
    "\n",
    "# Test different configurations if we have multiple GPUs\n",
    "if cpu_index is not None and num_gpus >= 2:\n",
    "    gpu_configs = test_gpu_configurations(cpu_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95978096",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-21T18:35:48.065511Z",
     "iopub.status.idle": "2025-08-21T18:35:48.065798Z",
     "shell.execute_reply": "2025-08-21T18:35:48.065668Z",
     "shell.execute_reply.started": "2025-08-21T18:35:48.065654Z"
    },
    "id": "95978096",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 8. Recall Evaluation with Synthetic Ground Truth (Optional)\n",
    "\n",
    "def create_synthetic_ground_truth(queries, index_size, relevant_per_query=100):\n",
    "    \"\"\"\n",
    "    Create synthetic ground truth for recall evaluation\n",
    "\n",
    "    Args:\n",
    "        queries: List of query strings\n",
    "        index_size: Total size of the index\n",
    "        relevant_per_query: Number of relevant documents per query\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping queries to relevant document indices\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    ground_truth = {}\n",
    "\n",
    "    for query in queries:\n",
    "        # Generate random relevant indices for each query\n",
    "        relevant_indices = np.random.choice(\n",
    "            index_size,\n",
    "            size=min(relevant_per_query, index_size),\n",
    "            replace=False\n",
    "        )\n",
    "        ground_truth[query] = relevant_indices.tolist()\n",
    "\n",
    "    return ground_truth\n",
    "\n",
    "def evaluate_recall_with_ground_truth(search_results, ground_truth, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate recall at different k values\n",
    "\n",
    "    Args:\n",
    "        search_results: Search results from search_multi_gpu\n",
    "        ground_truth: Dictionary mapping queries to relevant indices\n",
    "        k_values: List of k values to evaluate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with recall metrics\n",
    "    \"\"\"\n",
    "    recall_metrics = []\n",
    "\n",
    "    for i, query in enumerate(search_results['queries']):\n",
    "        if query not in ground_truth:\n",
    "            continue\n",
    "\n",
    "        retrieved_indices = search_results['indices'][i]\n",
    "        relevant_indices = ground_truth[query]\n",
    "\n",
    "        for k in k_values:\n",
    "            recall_at_k = calculate_recall_at_k(retrieved_indices, relevant_indices, k)\n",
    "            recall_metrics.append({\n",
    "                'query': query,\n",
    "                'k': k,\n",
    "                'recall': recall_at_k,\n",
    "                'relevant_docs': len(relevant_indices),\n",
    "                'retrieved_docs': min(k, len(retrieved_indices))\n",
    "            })\n",
    "\n",
    "    # Create DataFrame and compute averages\n",
    "    metrics_df = pd.DataFrame(recall_metrics)\n",
    "\n",
    "    # Compute average recall for each k\n",
    "    avg_recall = metrics_df.groupby('k')['recall'].mean().reset_index()\n",
    "    avg_recall.columns = ['k', 'avg_recall']\n",
    "\n",
    "    return metrics_df, avg_recall\n",
    "\n",
    "# Example: Evaluate with synthetic ground truth\n",
    "if cpu_index is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECALL EVALUATION WITH SYNTHETIC GROUND TRUTH\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Create synthetic ground truth\n",
    "    synthetic_gt = create_synthetic_ground_truth(\n",
    "        test_queries,\n",
    "        wikipedia_index.ntotal,\n",
    "        relevant_per_query=1000\n",
    "    )\n",
    "\n",
    "    print(f\"Created synthetic ground truth for {len(synthetic_gt)} queries\")\n",
    "    print(f\"Each query has {len(list(synthetic_gt.values())[0])} relevant documents\")\n",
    "\n",
    "    # Evaluate recall\n",
    "    detailed_metrics, avg_recall = evaluate_recall_with_ground_truth(\n",
    "        search_results,\n",
    "        synthetic_gt,\n",
    "        k_values=[1, 5, 10]\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"AVERAGE RECALL AT DIFFERENT K VALUES\")\n",
    "    print(\"-\"*40)\n",
    "    print(avg_recall.to_string(index=False))\n",
    "\n",
    "    # Plot recall curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(avg_recall['k'], avg_recall['avg_recall'], 'b-o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('K (Number of Retrieved Documents)')\n",
    "    plt.ylabel('Average Recall')\n",
    "    plt.title('Recall@K Curve for Multi-GPU FAISS Search')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim([0, max(avg_recall['avg_recall']) * 1.1])\n",
    "\n",
    "    # Add value labels\n",
    "    for k, recall in zip(avg_recall['k'], avg_recall['avg_recall']):\n",
    "        plt.annotate(f'{recall:.3f}',\n",
    "                    xy=(k, recall),\n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nNote: These are synthetic ground truth values for demonstration.\")\n",
    "    print(\"In production, use actual relevance labels for meaningful recall evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5005e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-21T18:35:48.066899Z",
     "iopub.status.idle": "2025-08-21T18:35:48.067204Z",
     "shell.execute_reply": "2025-08-21T18:35:48.067058Z",
     "shell.execute_reply.started": "2025-08-21T18:35:48.067044Z"
    },
    "id": "27d5005e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## 9. Resource Cleanup and Summary\n",
    "\n",
    "def cleanup_gpu_resources():\n",
    "    \"\"\"Clean up GPU resources\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"GPU cache cleared\")\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(\"Memory cleanup completed\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-GPU FAISS IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 KEY FEATURES IMPLEMENTED:\")\n",
    "print(\"  ✓ Multi-GPU FAISS index distribution (sharding)\")\n",
    "print(\"  ✓ Wikipedia 202307 index loading and search\")\n",
    "print(\"  ✓ Batch search optimization\")\n",
    "print(\"  ✓ Recall@K evaluation framework\")\n",
    "print(\"  ✓ Performance benchmarking\")\n",
    "print(\"  ✓ GPU configuration comparison (sharded vs replicated)\")\n",
    "\n",
    "if cpu_index is not None:\n",
    "    print(f\"\\n📈 PERFORMANCE METRICS:\")\n",
    "    print(f\"  • Index size: {wikipedia_index.ntotal:,} vectors\")\n",
    "    print(f\"  • Embedding dimension: {cpu_index.d}\")\n",
    "    print(f\"  • Number of GPUs utilized: {num_gpus}\")\n",
    "    print(f\"  • Average search latency: {search_results['avg_search_time_ms']:.2f} ms/query\")\n",
    "\n",
    "    if num_gpus > 0:\n",
    "        print(f\"\\n🔧 GPU CONFIGURATION:\")\n",
    "        print(f\"  • Configuration: Sharded (distributed) index\")\n",
    "        print(f\"  • Vectors per GPU: ~{wikipedia_index.ntotal // max(1, num_gpus):,}\")\n",
    "        print(f\"  • Memory efficiency: {num_gpus}x reduction vs replication\")\n",
    "\n",
    "print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(\"  1. Use sharded configuration for large indices (>1M vectors)\")\n",
    "print(\"  2. Batch queries for better throughput (10-20 queries optimal)\")\n",
    "print(\"  3. Monitor GPU memory usage for scaling decisions\")\n",
    "print(\"  4. Use recall evaluation to tune search parameters\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"  • Implement index updates and incremental indexing\")\n",
    "print(\"  • Add query result caching for frequently accessed queries\")\n",
    "print(\"  • Integrate with production serving framework\")\n",
    "print(\"  • Set up continuous monitoring and alerting\")\n",
    "\n",
    "# Cleanup\n",
    "print(\"\\n🧹 Cleaning up resources...\")\n",
    "cleanup_gpu_resources()\n",
    "print_memory_status(\"Final after cleanup\")\n",
    "\n",
    "print(\"\\n✅ Multi-GPU FAISS implementation completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 3524699,
     "sourceId": 6146317,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
